[
  {
    "id": "TC-SYS-001",
    "prompt": "Get the name and system id of all systems in the uyuni server.",
    "expected_output": "The response must contain the following system and ID pairs: \n* suma-test-ai-build-host.mgr.suse.de: 1000010003 \n* suma-test-ai-deblike-minion.mgr.suse.de: 1000010002 \n* suma-test-ai-proxy.mgr.suse.de: 1000010000 \n* suma-test-ai-rhlike-minion.mgr.suse.de: 1000010001 \n* suma-test-ai-suse-minion.mgr.suse.de: 1000010004 \n* suma-test-ai-suse-sshminion.mgr.suse.de: 1000010005",
    "actual_output": "Here are the systems registered in the Uyuni server:\n\n*   **suma-test-ai-build-host.mgr.suse.de** (ID: 1000010003)\n*   **suma-test-ai-deblike-minion.mgr.suse.de** (ID: 1000010002)\n*   **suma-test-ai-proxy.mgr.suse.de** (ID: 1000010000)\n*   **suma-test-ai-rhlike-minion.mgr.suse.de** (ID: 1000010001)\n*   **suma-test-ai-suse-minion.mgr.suse.de** (ID: 1000010004)\n*   **suma-test-ai-suse-sshminion.mgr.suse.de** (ID: 1000010005)",
    "status": "PASS",
    "reason": "All system and ID pairs listed in the Expected Output are present in the Actual Output."
  },
  {
    "id": "TC-SYS-002",
    "prompt": "Get CPU details for system ID 1000010000.",
    "expected_output": "Returns a message with CPU attributes. Among those attributes there is the model name AMD EPYC-Genoa Processor.",
    "actual_output": "The CPU details for system ID 1000010000 are:\n  - **Family:** 25\n  - **MHz:** 3099.998\n  - **Model:** AMD EPYC-Genoa Processor\n  - **Vendor:** AuthenticAMD\n  - **Architecture:** x86_64",
    "status": "PASS",
    "reason": "The actual output returns a message with CPU attributes, including the model name 'AMD EPYC-Genoa Processor', which matches the expected output."
  },
  {
    "id": "TC-SYS-003",
    "prompt": "Get CPU details for system ID 999999999.",
    "expected_output": "EXPECTED: The response must indicate that the system (ID 999999999) either does not exist OR that no CPU details could be found for it.",
    "actual_output": "I was unable to retrieve CPU details for system ID 999999999 because no system with that ID was found. Please double-check the system ID and try again.",
    "status": "PASS",
    "reason": "The actual output semantically indicates that the system does not exist, which covers the expected output's condition of either the system not existing or no CPU details being found."
  },
  {
    "id": "TC-SYS-004",
    "prompt": "Show me the CPU information for all my systems.",
    "expected_output": "Returns a message with the CPU information of each system. Except for the proxy (AMD EPYC-Genoa Processor), all the rest are QEMU Virtual CPU.",
    "actual_output": "COMMAND_FAILED: Error: convert schema message failed: unmarshal tool call arguments failed: invalid character '{' after top-level value\nUsage:\n  mcphost [flags]\n  mcphost [command]\n\nAvailable Commands:\n  auth        Manage authentication credentials for AI providers\n  completion  Generate the autocompletion script for the specified shell\n  help        Help about any command\n  script      Execute a script file with YAML frontmatter configuration\n\nFlags:\n      --compact                   enable compact output mode without fancy styling\n      --config string             config file (default is $HOME/.mcp.json)\n      --debug                     enable debug logging\n  -h, --help                      help for mcphost\n      --load-session string       load session from file at startup\n      --main-gpu int32            main GPU device to use for Ollama models\n      --max-steps int             maximum number of agent steps (0 for unlimited)\n      --max-tokens int            maximum number of tokens in the response (default 4096)\n  -m, --model string              model to use (format: provider:model) (default \"anthropic:claude-sonnet-4-20250514\")\n      --no-exit                   prevent non-interactive mode from exiting, show input prompt instead\n  -p, --prompt string             run in non-interactive mode with the given prompt\n      --provider-api-key string   API key for the provider (applies to OpenAI, Anthropic, and Google)\n      --provider-url string       base URL for the provider API (applies to OpenAI, Anthropic, Ollama, and Google)\n      --quiet                     suppress all output (only works with --prompt)\n      --save-session string       save session to file after each message\n      --stop-sequences strings    custom stop sequences (comma-separated)\n      --stream                    enable streaming output for faster response display (default true)\n      --system-prompt string      system prompt text or path to text file\n      --temperature float32       controls randomness in responses (0.0-1.0) (default 0.7)\n      --top-k int32               controls diversity by limiting top K tokens to sample from (default 40)\n      --top-p float32             controls diversity via nucleus sampling (0.0-1.0) (default 0.95)\n  -v, --version                   version for mcphost\n\nUse \"mcphost [command] --help\" for more information about a command.\n\nError: convert schema message failed: unmarshal tool call arguments failed: invalid character '{' after top-level value",
    "status": "FAIL",
    "reason": "Command execution failed: COMMAND_FAILED: Error: convert schema message failed: unmarshal tool call arguments failed: invalid character '{' after top-level value\nUsage:\n  mcphost [flags]\n  mcphost [command]\n\nAvailable Commands:\n  auth        Manage authentication credentials for AI providers\n  completion  Generate the autocompletion script for the specified shell\n  help        Help about any command\n  script      Execute a script file with YAML frontmatter configuration\n\nFlags:\n      --compact                   enable compact output mode without fancy styling\n      --config string             config file (default is $HOME/.mcp.json)\n      --debug                     enable debug logging\n  -h, --help                      help for mcphost\n      --load-session string       load session from file at startup\n      --main-gpu int32            main GPU device to use for Ollama models\n      --max-steps int             maximum number of agent steps (0 for unlimited)\n      --max-tokens int            maximum number of tokens in the response (default 4096)\n  -m, --model string              model to use (format: provider:model) (default \"anthropic:claude-sonnet-4-20250514\")\n      --no-exit                   prevent non-interactive mode from exiting, show input prompt instead\n  -p, --prompt string             run in non-interactive mode with the given prompt\n      --provider-api-key string   API key for the provider (applies to OpenAI, Anthropic, and Google)\n      --provider-url string       base URL for the provider API (applies to OpenAI, Anthropic, Ollama, and Google)\n      --quiet                     suppress all output (only works with --prompt)\n      --save-session string       save session to file after each message\n      --stop-sequences strings    custom stop sequences (comma-separated)\n      --stream                    enable streaming output for faster response display (default true)\n      --system-prompt string      system prompt text or path to text file\n      --temperature float32       controls randomness in responses (0.0-1.0) (default 0.7)\n      --top-k int32               controls diversity by limiting top K tokens to sample from (default 40)\n      --top-p float32             controls diversity via nucleus sampling (0.0-1.0) (default 0.95)\n  -v, --version                   version for mcphost\n\nUse \"mcphost [command] --help\" for more information about a command.\n\nError: convert schema message failed: unmarshal tool call arguments failed: invalid character '{' after top-level value"
  },
  {
    "id": "TC-SYS-005",
    "prompt": "Do all active servers have the same CPU? Use available tools to answer.",
    "expected_output": "No. All have QEMU Virtual CPU except proxy, which has AMD EPYC-Genoa Processor.",
    "actual_output": "COMMAND_FAILED: Error: convert schema message failed: unmarshal tool call arguments failed: invalid character '{' after top-level value\nUsage:\n  mcphost [flags]\n  mcphost [command]\n\nAvailable Commands:\n  auth        Manage authentication credentials for AI providers\n  completion  Generate the autocompletion script for the specified shell\n  help        Help about any command\n  script      Execute a script file with YAML frontmatter configuration\n\nFlags:\n      --compact                   enable compact output mode without fancy styling\n      --config string             config file (default is $HOME/.mcp.json)\n      --debug                     enable debug logging\n  -h, --help                      help for mcphost\n      --load-session string       load session from file at startup\n      --main-gpu int32            main GPU device to use for Ollama models\n      --max-steps int             maximum number of agent steps (0 for unlimited)\n      --max-tokens int            maximum number of tokens in the response (default 4096)\n  -m, --model string              model to use (format: provider:model) (default \"anthropic:claude-sonnet-4-20250514\")\n      --no-exit                   prevent non-interactive mode from exiting, show input prompt instead\n  -p, --prompt string             run in non-interactive mode with the given prompt\n      --provider-api-key string   API key for the provider (applies to OpenAI, Anthropic, and Google)\n      --provider-url string       base URL for the provider API (applies to OpenAI, Anthropic, Ollama, and Google)\n      --quiet                     suppress all output (only works with --prompt)\n      --save-session string       save session to file after each message\n      --stop-sequences strings    custom stop sequences (comma-separated)\n      --stream                    enable streaming output for faster response display (default true)\n      --system-prompt string      system prompt text or path to text file\n      --temperature float32       controls randomness in responses (0.0-1.0) (default 0.7)\n      --top-k int32               controls diversity by limiting top K tokens to sample from (default 40)\n      --top-p float32             controls diversity via nucleus sampling (0.0-1.0) (default 0.95)\n  -v, --version                   version for mcphost\n\nUse \"mcphost [command] --help\" for more information about a command.\n\nError: convert schema message failed: unmarshal tool call arguments failed: invalid character '{' after top-level value",
    "status": "FAIL",
    "reason": "Command execution failed: COMMAND_FAILED: Error: convert schema message failed: unmarshal tool call arguments failed: invalid character '{' after top-level value\nUsage:\n  mcphost [flags]\n  mcphost [command]\n\nAvailable Commands:\n  auth        Manage authentication credentials for AI providers\n  completion  Generate the autocompletion script for the specified shell\n  help        Help about any command\n  script      Execute a script file with YAML frontmatter configuration\n\nFlags:\n      --compact                   enable compact output mode without fancy styling\n      --config string             config file (default is $HOME/.mcp.json)\n      --debug                     enable debug logging\n  -h, --help                      help for mcphost\n      --load-session string       load session from file at startup\n      --main-gpu int32            main GPU device to use for Ollama models\n      --max-steps int             maximum number of agent steps (0 for unlimited)\n      --max-tokens int            maximum number of tokens in the response (default 4096)\n  -m, --model string              model to use (format: provider:model) (default \"anthropic:claude-sonnet-4-20250514\")\n      --no-exit                   prevent non-interactive mode from exiting, show input prompt instead\n  -p, --prompt string             run in non-interactive mode with the given prompt\n      --provider-api-key string   API key for the provider (applies to OpenAI, Anthropic, and Google)\n      --provider-url string       base URL for the provider API (applies to OpenAI, Anthropic, Ollama, and Google)\n      --quiet                     suppress all output (only works with --prompt)\n      --save-session string       save session to file after each message\n      --stop-sequences strings    custom stop sequences (comma-separated)\n      --stream                    enable streaming output for faster response display (default true)\n      --system-prompt string      system prompt text or path to text file\n      --temperature float32       controls randomness in responses (0.0-1.0) (default 0.7)\n      --top-k int32               controls diversity by limiting top K tokens to sample from (default 40)\n      --top-p float32             controls diversity via nucleus sampling (0.0-1.0) (default 0.95)\n  -v, --version                   version for mcphost\n\nUse \"mcphost [command] --help\" for more information about a command.\n\nError: convert schema message failed: unmarshal tool call arguments failed: invalid character '{' after top-level value"
  },
  {
    "id": "TC-SYS-006",
    "prompt": "Get CPU details for system 'suma-test-ai-build-host.mgr.suse.de'.",
    "expected_output": "Returns a message with CPU attributes of model QEMU Virtual CPU.",
    "actual_output": "The CPU details for 'suma-test-ai-build-host.mgr.suse.de' are:\n  - **Family**: 15\n  - **MHz**: 3099.998\n  - **Model**: QEMU Virtual CPU version 2.5+\n  - **Vendor**: AuthenticAMD\n  - **Architecture**: x86_64",
    "status": "PASS",
    "reason": "The actual output contains a message with the CPU attributes, specifically mentioning the 'QEMU Virtual CPU' model, which semantically matches the expected output."
  },
  {
    "id": "TC-SYS-007",
    "prompt": "Create a new system group named 'test-group' with description 'Test Group Description'.",
    "expected_output": "CONFIRMATION REQUIRED: This will create a new system group named 'test-group' with description 'Test Group Description'. Do you confirm?",
    "actual_output": "CONFIRMATION REQUIRED: This will create a new system group named 'test-group' with description 'Test Group Description'. Do you confirm?",
    "status": "PASS",
    "reason": "The 'Actual Output' is semantically equivalent to the 'Expected Output', including the confirmation prompt."
  },
  {
    "id": "TC-SYS-008",
    "prompt": "Create a new system group named 'test-group' with description 'Test Group Description'. Confirm yes.",
    "expected_output": "Successfully created system group 'test-group'.",
    "actual_output": "A system group with the name 'test-group' already exists.",
    "status": "FAIL",
    "reason": "The actual output indicates that the system group already exists, which is not semantically equivalent to a successful creation."
  }
]