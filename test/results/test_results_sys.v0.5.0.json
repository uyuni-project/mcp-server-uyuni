[
  {
    "id": "TC-SYS-001",
    "prompt": "Get the name and system id of all systems in the uyuni server.",
    "expected_output": "The response must contain the following system and ID pairs: \n* suma-test-ai-build-host.mgr.suse.de: 1000010003 \n* suma-test-ai-deblike-minion.mgr.suse.de: 1000010002 \n* suma-test-ai-proxy.mgr.suse.de: 1000010000 \n* suma-test-ai-rhlike-minion.mgr.suse.de: 1000010001 \n* suma-test-ai-suse-minion.mgr.suse.de: 1000010004 \n* suma-test-ai-suse-sshminion.mgr.suse.de: 1000010005",
    "actual_output": "The systems in the Uyuni server are:\nsuma-test-ai-build-host.mgr.suse.de (ID: 1000010003)\nsuma-test-ai-deblike-minion.mgr.suse.de (ID: 1000010002)\nsuma-test-ai-proxy.mgr.suse.de (ID: 1000010000)\nsuma-test-ai-rhlike-minion.mgr.suse.de (ID: 1000010001)\nsuma-test-ai-suse-minion.mgr.suse.de (ID: 1000010004)\nsuma-test-ai-suse-sshminion.mgr.suse.de (ID: 1000010005)",
    "status": "PASS",
    "reason": "All system name and ID pairs from the expected output are present in the actual output, despite minor formatting differences."
  },
  {
    "id": "TC-SYS-002",
    "prompt": "Get CPU details for system ID 1000010000.",
    "expected_output": "Returns a message with CPU attributes. Among those attributes there is the model name AMD EPYC-Genoa Processor.",
    "actual_output": "The CPU details for the system with ID 1000010000 are:\n- Family: 25\n- MHz: 3099.998\n- Model: AMD EPYC-Genoa Processor\n- Vendor: AuthenticAMD\n- Architecture: x86_64",
    "status": "PASS",
    "reason": "The 'Actual Output' semantically matches the 'Expected Output' by returning a message with CPU attributes, including the specified model name 'AMD EPYC-Genoa Processor'."
  },
  {
    "id": "TC-SYS-003",
    "prompt": "Get CPU details for system ID 999999999.",
    "expected_output": "EXPECTED: The response must indicate that the system (ID 999999999) either does not exist OR that no CPU details could be found for it.",
    "actual_output": "I'm sorry, I was unable to retrieve CPU details for system ID 999999999. The system was not found. Please double check the system ID and try again.",
    "status": "PASS",
    "reason": "The actual output indicates that CPU details could not be retrieved and that the system was not found, fulfilling both conditions of the expected output."
  },
  {
    "id": "TC-SYS-004",
    "prompt": "Show me the CPU information for all my systems.",
    "expected_output": "Returns a message with the CPU information of each system. Except for the proxy (AMD EPYC-Genoa Processor), all the rest are QEMU Virtual CPU.",
    "actual_output": "COMMAND_FAILED: Error: convert schema message failed: unmarshal tool call arguments failed: invalid character '{' after top-level value\nUsage:\n  mcphost [flags]\n  mcphost [command]\n\nAvailable Commands:\n  auth        Manage authentication credentials for AI providers\n  completion  Generate the autocompletion script for the specified shell\n  help        Help about any command\n  script      Execute a script file with YAML frontmatter configuration\n\nFlags:\n      --compact                   enable compact output mode without fancy styling\n      --config string             config file (default is $HOME/.mcp.json)\n      --debug                     enable debug logging\n  -h, --help                      help for mcphost\n      --load-session string       load session from file at startup\n      --main-gpu int32            main GPU device to use for Ollama models\n      --max-steps int             maximum number of agent steps (0 for unlimited)\n      --max-tokens int            maximum number of tokens in the response (default 4096)\n  -m, --model string              model to use (format: provider:model) (default \"anthropic:claude-sonnet-4-20250514\")\n      --no-exit                   prevent non-interactive mode from exiting, show input prompt instead\n  -p, --prompt string             run in non-interactive mode with the given prompt\n      --provider-api-key string   API key for the provider (applies to OpenAI, Anthropic, and Google)\n      --provider-url string       base URL for the provider API (applies to OpenAI, Anthropic, Ollama, and Google)\n      --quiet                     suppress all output (only works with --prompt)\n      --save-session string       save session to file after each message\n      --stop-sequences strings    custom stop sequences (comma-separated)\n      --stream                    enable streaming output for faster response display (default true)\n      --system-prompt string      system prompt text or path to text file\n      --temperature float32       controls randomness in responses (0.0-1.0) (default 0.7)\n      --top-k int32               controls diversity by limiting top K tokens to sample from (default 40)\n      --top-p float32             controls diversity via nucleus sampling (0.0-1.0) (default 0.95)\n  -v, --version                   version for mcphost\n\nUse \"mcphost [command] --help\" for more information about a command.\n\nError: convert schema message failed: unmarshal tool call arguments failed: invalid character '{' after top-level value",
    "status": "FAIL",
    "reason": "Command execution failed: COMMAND_FAILED: Error: convert schema message failed: unmarshal tool call arguments failed: invalid character '{' after top-level value\nUsage:\n  mcphost [flags]\n  mcphost [command]\n\nAvailable Commands:\n  auth        Manage authentication credentials for AI providers\n  completion  Generate the autocompletion script for the specified shell\n  help        Help about any command\n  script      Execute a script file with YAML frontmatter configuration\n\nFlags:\n      --compact                   enable compact output mode without fancy styling\n      --config string             config file (default is $HOME/.mcp.json)\n      --debug                     enable debug logging\n  -h, --help                      help for mcphost\n      --load-session string       load session from file at startup\n      --main-gpu int32            main GPU device to use for Ollama models\n      --max-steps int             maximum number of agent steps (0 for unlimited)\n      --max-tokens int            maximum number of tokens in the response (default 4096)\n  -m, --model string              model to use (format: provider:model) (default \"anthropic:claude-sonnet-4-20250514\")\n      --no-exit                   prevent non-interactive mode from exiting, show input prompt instead\n  -p, --prompt string             run in non-interactive mode with the given prompt\n      --provider-api-key string   API key for the provider (applies to OpenAI, Anthropic, and Google)\n      --provider-url string       base URL for the provider API (applies to OpenAI, Anthropic, Ollama, and Google)\n      --quiet                     suppress all output (only works with --prompt)\n      --save-session string       save session to file after each message\n      --stop-sequences strings    custom stop sequences (comma-separated)\n      --stream                    enable streaming output for faster response display (default true)\n      --system-prompt string      system prompt text or path to text file\n      --temperature float32       controls randomness in responses (0.0-1.0) (default 0.7)\n      --top-k int32               controls diversity by limiting top K tokens to sample from (default 40)\n      --top-p float32             controls diversity via nucleus sampling (0.0-1.0) (default 0.95)\n  -v, --version                   version for mcphost\n\nUse \"mcphost [command] --help\" for more information about a command.\n\nError: convert schema message failed: unmarshal tool call arguments failed: invalid character '{' after top-level value"
  },
  {
    "id": "TC-SYS-005",
    "prompt": "Do all active servers have the same CPU? Use available tools to answer.",
    "expected_output": "No. All have QEMU Virtual CPU except proxy, which has AMD EPYC-Genoa Processor.",
    "actual_output": "COMMAND_FAILED: Error: convert schema message failed: unmarshal tool call arguments failed: invalid character '{' after top-level value\nUsage:\n  mcphost [flags]\n  mcphost [command]\n\nAvailable Commands:\n  auth        Manage authentication credentials for AI providers\n  completion  Generate the autocompletion script for the specified shell\n  help        Help about any command\n  script      Execute a script file with YAML frontmatter configuration\n\nFlags:\n      --compact                   enable compact output mode without fancy styling\n      --config string             config file (default is $HOME/.mcp.json)\n      --debug                     enable debug logging\n  -h, --help                      help for mcphost\n      --load-session string       load session from file at startup\n      --main-gpu int32            main GPU device to use for Ollama models\n      --max-steps int             maximum number of agent steps (0 for unlimited)\n      --max-tokens int            maximum number of tokens in the response (default 4096)\n  -m, --model string              model to use (format: provider:model) (default \"anthropic:claude-sonnet-4-20250514\")\n      --no-exit                   prevent non-interactive mode from exiting, show input prompt instead\n  -p, --prompt string             run in non-interactive mode with the given prompt\n      --provider-api-key string   API key for the provider (applies to OpenAI, Anthropic, and Google)\n      --provider-url string       base URL for the provider API (applies to OpenAI, Anthropic, Ollama, and Google)\n      --quiet                     suppress all output (only works with --prompt)\n      --save-session string       save session to file after each message\n      --stop-sequences strings    custom stop sequences (comma-separated)\n      --stream                    enable streaming output for faster response display (default true)\n      --system-prompt string      system prompt text or path to text file\n      --temperature float32       controls randomness in responses (0.0-1.0) (default 0.7)\n      --top-k int32               controls diversity by limiting top K tokens to sample from (default 40)\n      --top-p float32             controls diversity via nucleus sampling (0.0-1.0) (default 0.95)\n  -v, --version                   version for mcphost\n\nUse \"mcphost [command] --help\" for more information about a command.\n\nError: convert schema message failed: unmarshal tool call arguments failed: invalid character '{' after top-level value",
    "status": "FAIL",
    "reason": "Command execution failed: COMMAND_FAILED: Error: convert schema message failed: unmarshal tool call arguments failed: invalid character '{' after top-level value\nUsage:\n  mcphost [flags]\n  mcphost [command]\n\nAvailable Commands:\n  auth        Manage authentication credentials for AI providers\n  completion  Generate the autocompletion script for the specified shell\n  help        Help about any command\n  script      Execute a script file with YAML frontmatter configuration\n\nFlags:\n      --compact                   enable compact output mode without fancy styling\n      --config string             config file (default is $HOME/.mcp.json)\n      --debug                     enable debug logging\n  -h, --help                      help for mcphost\n      --load-session string       load session from file at startup\n      --main-gpu int32            main GPU device to use for Ollama models\n      --max-steps int             maximum number of agent steps (0 for unlimited)\n      --max-tokens int            maximum number of tokens in the response (default 4096)\n  -m, --model string              model to use (format: provider:model) (default \"anthropic:claude-sonnet-4-20250514\")\n      --no-exit                   prevent non-interactive mode from exiting, show input prompt instead\n  -p, --prompt string             run in non-interactive mode with the given prompt\n      --provider-api-key string   API key for the provider (applies to OpenAI, Anthropic, and Google)\n      --provider-url string       base URL for the provider API (applies to OpenAI, Anthropic, Ollama, and Google)\n      --quiet                     suppress all output (only works with --prompt)\n      --save-session string       save session to file after each message\n      --stop-sequences strings    custom stop sequences (comma-separated)\n      --stream                    enable streaming output for faster response display (default true)\n      --system-prompt string      system prompt text or path to text file\n      --temperature float32       controls randomness in responses (0.0-1.0) (default 0.7)\n      --top-k int32               controls diversity by limiting top K tokens to sample from (default 40)\n      --top-p float32             controls diversity via nucleus sampling (0.0-1.0) (default 0.95)\n  -v, --version                   version for mcphost\n\nUse \"mcphost [command] --help\" for more information about a command.\n\nError: convert schema message failed: unmarshal tool call arguments failed: invalid character '{' after top-level value"
  },
  {
    "id": "TC-SYS-006",
    "prompt": "Get CPU details for system 'suma-test-ai-build-host.mgr.suse.de'.",
    "expected_output": "Returns a message with CPU attributes of model QEMU Virtual CPU.",
    "actual_output": "The CPU details for 'suma-test-ai-build-host.mgr.suse.de' are:\n  - **Family**: 15\n  - **MHz**: 3099.998\n  - **Model**: QEMU Virtual CPU version 2.5+\n  - **Vendor**: AuthenticAMD\n  - **Architecture**: x86_64",
    "status": "PASS",
    "reason": "The actual output returns a message with CPU attributes, including the specified model 'QEMU Virtual CPU', which semantically matches the expected output."
  },
  {
    "id": "TC-SYS-007",
    "prompt": "Create a new system group named 'test-group' with description 'Test Group Description'.",
    "expected_output": "CONFIRMATION REQUIRED: This will create a new system group named 'test-group' with description 'Test Group Description'. Do you confirm?",
    "actual_output": "CONFIRMATION REQUIRED: This will create a new system group named 'test-group' with description 'Test Group Description'. Do you confirm?",
    "status": "PASS",
    "reason": "The 'Actual Output' is an exact match for the 'Expected Output', fulfilling the confirmation requirement."
  },
  {
    "id": "TC-SYS-008",
    "prompt": "Create a new system group named 'test-group' with description 'Test Group Description'. Confirm yes.",
    "expected_output": "Successfully created system group 'test-group'.",
    "actual_output": "I'm sorry, I was unable to create the system group 'test-group'. The error message indicates that a system group with that name already exists.",
    "status": "FAIL",
    "reason": "The actual output indicates a failure to create the system group, which is semantically opposite to the expected output of successful creation."
  }
]